{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WqBOf2ipWg6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6de71760"
      },
      "source": [
        "# Task\n",
        "Train a GPT-2 model on a custom dataset to generate text based on a given prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaab0da2"
      },
      "source": [
        "## Set up the environment\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary libraries, including `transformers` and `torch`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7e5b912"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3a9a2b1",
        "outputId": "b54ef184-f2b2-4a5f-c3a5-78e0baa19dea"
      },
      "source": [
        "%pip install transformers torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fbb4f34"
      },
      "source": [
        "## Load and prepare the dataset\n",
        "\n",
        "### Subtask:\n",
        "Load your custom dataset and format it for training the GPT-2 model. This may involve tokenization and creating input sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ce7990"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the dataset. I will define the path to a dummy dataset file, create the file with some sample data, and then load the data into a pandas DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f40b1357",
        "outputId": "4709fccc-15a1-451b-b448-896611eef80d"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to your custom dataset file\n",
        "dataset_path = 'custom_dataset.txt'\n",
        "\n",
        "# Create a dummy dataset file for demonstration\n",
        "dummy_data = [\"This is the first sentence.\", \"Here is the second sentence.\", \"And a third one for testing.\"]\n",
        "with open(dataset_path, 'w') as f:\n",
        "    for line in dummy_data:\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_csv(dataset_path, header=None, names=['text'])\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27dc3eb4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is loaded, I need to tokenize it using a GPT-2 tokenizer and prepare the input sequences for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "11f283ac",
        "outputId": "5a8746b3-3aa8-4168-b74b-fd635e0ed3e2"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Initialize a GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Set the padding token if it's not in the tokenizer\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_data = tokenizer(\n",
        "    df['text'].tolist(),\n",
        "    return_tensors='pt',\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Prepare the tokenized data as input sequences\n",
        "input_ids = tokenized_data['input_ids']\n",
        "attention_mask = tokenized_data['attention_mask']\n",
        "\n",
        "print(\"Input IDs shape:\", input_ids.shape)\n",
        "print(\"Attention Mask shape:\", attention_mask.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "708de0af"
      },
      "source": [
        "## Load the pre-trained gpt-2 model\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained GPT-2 model and tokenizer from the `transformers` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2c1de5a"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the pre-trained GPT-2 model and resize its token embeddings if the tokenizer was resized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "f1ba0513",
        "outputId": "781c47ba-cdd1-4ecf-8e04-47dd53c4bf38"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Resize the model's token embeddings if the tokenizer was resized\n",
        "if tokenizer.pad_token is not None and len(tokenizer) != model.get_input_embeddings().num_embeddings:\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"GPT-2 model loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d157b8c3"
      },
      "source": [
        "## Configure the training parameters\n",
        "\n",
        "### Subtask:\n",
        "Set up the training arguments, such as the number of epochs, batch size, learning rate, and output directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5c5790b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the `TrainingArguments` class and instantiate it with the specified parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ad4daed",
        "outputId": "eeabd688-eaa9-4bb5-d33e-4ef881055d28"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define the output directory for checkpoints and logs\n",
        "output_dir = './results'\n",
        "\n",
        "# Instantiate TrainingArguments with report_to set to \"none\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=2,  # Batch size for training\n",
        "    save_steps=10_000,  # Save model every 10,000 steps\n",
        "    logging_steps=1000,  # Log every 1,000 steps\n",
        "    report_to=\"none\" # Disable reporting to integrations like wandb\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured successfully.\")\n",
        "print(training_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3a4905c"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "### Subtask:\n",
        "Train the GPT-2 model on your custom dataset using the configured training parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e24ce817"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a custom dataset class to hold the tokenized data and instantiate the Trainer with the model, training arguments, and the custom dataset, then start the training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "1Vv_N9EvaoQK",
        "outputId": "bff401e1-fa19-4b80-e583-4e6d894131f5"
      },
      "source": [
        "from transformers import Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Create a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'labels': self.input_ids[idx]}\n",
        "\n",
        "# Instantiate the custom dataset\n",
        "train_dataset = CustomDataset(input_ids, attention_mask)\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "563fa006"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the fine-tuned model using relevant metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca882380"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the evaluation dataset and call the evaluate method of the trainer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7b1d30de",
        "outputId": "3ec4c739-8430-42db-91f7-e16bcef9815b"
      },
      "source": [
        "# Define an evaluation dataset. For simplicity, use the same train_dataset.\n",
        "eval_dataset = train_dataset\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation_results = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(evaluation_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ad0c7a"
      },
      "source": [
        "## Generate text\n",
        "\n",
        "### Subtask:\n",
        "Use the fine-tuned model to generate text based on a given prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78cd0f8"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a text generation pipeline, define a prompt, and generate text using the fine-tuned model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fea16293",
        "outputId": "cc7ea80e-b9fc-4fb7-e6ad-7063d02501a7"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Define a prompt string\n",
        "prompt = \"This is a test\"\n",
        "\n",
        "# Generate text based on the prompt\n",
        "generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text[0]['generated_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3493f398"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The necessary libraries (`transformers` and `torch`) were successfully installed.\n",
        "*   A custom dataset was successfully loaded and tokenized using the GPT-2 tokenizer, preparing it for training as input IDs and attention masks in PyTorch tensors.\n",
        "*   A pre-trained GPT-2 model (`gpt2`) was successfully loaded.\n",
        "*   Training arguments were configured, including the output directory (`./results`), number of epochs (3), batch size (2), and logging/saving steps.\n",
        "*   The GPT-2 model was successfully fine-tuned on the custom dataset using the configured `TrainingArguments`.\n",
        "*   The model was evaluated using the training dataset, yielding an `eval_loss` of approximately 3.175.\n",
        "*   The fine-tuned model successfully generated text based on a given prompt using a text generation pipeline.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The model's performance should be evaluated on a separate validation or test dataset to get a more objective measure of its generalization capabilities.\n",
        "*   Further fine-tuning experiments could be conducted by adjusting hyperparameters like the learning rate, batch size, or number of epochs to potentially improve the evaluation loss and generated text quality.\n"
      ]
    }
  ]
}
