{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA0bPLCV+/W4aLjeOfh36N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandanpujan/Gen-Ai/blob/main/TextGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MWGG8x2TjoR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1885938"
      },
      "source": [
        "# Task\n",
        "Implement a simple text generation algorithm using Markov chains. This task involves creating a statistical model that predicts the probability of a character or word based on the previous one(s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a12220"
      },
      "source": [
        "## Prepare the training data\n",
        "\n",
        "### Subtask:\n",
        "Load a text corpus and preprocess it by cleaning the text and splitting it into a sequence of tokens (characters or words).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aefbf3af"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the text data, convert to lowercase, remove punctuation, and split into words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "e3ed132c",
        "outputId": "939b4c78-10c6-42cd-e02b-dc9a84f5aef3"
      },
      "source": [
        "import re\n",
        "\n",
        "file_path = '/tmp/text_corpus.txt'\n",
        "with open(file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "words = text.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/tmp/text_corpus.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-395672758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tmp/text_corpus.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/text_corpus.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091bee6c"
      },
      "source": [
        "## Build the markov chain model\n",
        "\n",
        "### Subtask:\n",
        "Create a dictionary or similar data structure to store the transition probabilities of tokens based on the preceding token(s).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7db0acf7"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the previous subtask failed due to a missing file, I need to create a dummy text file to proceed with the current subtask of building the Markov chain. Then, I will initialize the markov chain dictionary and populate it with word transitions and their counts based on the dummy text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ba2157",
        "outputId": "f532c9c5-e995-44ec-e468-31e94f4006b2"
      },
      "source": [
        "import os\n",
        "\n",
        "file_path = '/tmp/text_corpus.txt'\n",
        "dummy_text = \"this is a test text this is another test\"\n",
        "with open(file_path, 'w') as f:\n",
        "    f.write(dummy_text)\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "words = text.split()\n",
        "\n",
        "markov_chain = {}\n",
        "\n",
        "for i in range(len(words)):\n",
        "    current_word = words[i]\n",
        "    if i > 0:\n",
        "        prev_word = words[i-1]\n",
        "        if prev_word not in markov_chain:\n",
        "            markov_chain[prev_word] = {}\n",
        "        if current_word not in markov_chain[prev_word]:\n",
        "            markov_chain[prev_word][current_word] = 0\n",
        "        markov_chain[prev_word][current_word] += 1\n",
        "    else:\n",
        "        # Handle the start of the text, use a special token or None\n",
        "        if None not in markov_chain:\n",
        "            markov_chain[None] = {}\n",
        "        if current_word not in markov_chain[None]:\n",
        "            markov_chain[None][current_word] = 0\n",
        "        markov_chain[None][current_word] += 1\n",
        "\n",
        "# Convert counts to probabilities\n",
        "for prev_word, next_words in markov_chain.items():\n",
        "    total_count = sum(next_words.values())\n",
        "    for next_word, count in next_words.items():\n",
        "        markov_chain[prev_word][next_word] = count / total_count\n",
        "\n",
        "print(markov_chain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{None: {'this': 1.0}, 'this': {'is': 1.0}, 'is': {'a': 0.5, 'another': 0.5}, 'a': {'test': 1.0}, 'test': {'text': 1.0}, 'text': {'this': 1.0}, 'another': {'test': 1.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90adc594"
      },
      "source": [
        "## Generate text\n",
        "\n",
        "### Subtask:\n",
        "Start with an initial token and iteratively generate subsequent tokens based on the probabilities in the Markov chain model until a desired length is reached.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf61219e"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `generate_text` function to generate text using the Markov chain and then call the function and print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d7b7a11",
        "outputId": "f5e349e1-afe7-4670-96c5-00cc5ac1fab9"
      },
      "source": [
        "import random\n",
        "\n",
        "def generate_text(markov_chain, length):\n",
        "    current_word = random.choices(list(markov_chain[None].keys()), weights=list(markov_chain[None].values()), k=1)[0]\n",
        "    generated_text = [current_word]\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        if current_word in markov_chain:\n",
        "            next_words = markov_chain[current_word]\n",
        "            if next_words:\n",
        "                current_word = random.choices(list(next_words.keys()), weights=list(next_words.values()), k=1)[0]\n",
        "                generated_text.append(current_word)\n",
        "            else:\n",
        "                # If there are no next words, stop or choose a new starting word\n",
        "                # For simplicity, let's stop here\n",
        "                break\n",
        "        else:\n",
        "            # If the current word is not in the markov chain, stop or choose a new starting word\n",
        "            # For simplicity, let's stop here\n",
        "            break\n",
        "\n",
        "    return \" \".join(generated_text)\n",
        "\n",
        "generated_text_output = generate_text(markov_chain, 20)\n",
        "print(generated_text_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a test text this is a test text this is a test text this is another test text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "648cf25f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to load the text corpus resulted in a `FileNotFoundError` because the file was not present at the specified path (`/tmp/text_corpus.txt`).\n",
        "*   A dummy text file with the content \"this is a test text this is another test\" was created to enable further steps.\n",
        "*   The text was successfully preprocessed by converting it to lowercase, removing non-alphabetical characters (except spaces), and splitting it into a list of words.\n",
        "*   A first-order Markov chain model was built as a dictionary. This dictionary stores the transition probabilities between words, using `None` to represent the start of the text.\n",
        "*   The transition counts were successfully converted into probabilities by dividing the count of each next word by the total count for the preceding word.\n",
        "*   A function `generate_text` was implemented to generate text of a specified length using the built Markov chain model.\n",
        "*   The text generation starts with a word chosen probabilistically from those that can follow `None` (the start of the text).\n",
        "*   Subsequent words are chosen probabilistically based on the preceding word's transitions defined in the Markov chain.\n",
        "*   The generated text output, \"this is a test text this is a test text this is a test text this is another test text\", demonstrated the algorithm's ability to produce sequences based on the learned probabilities.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current model is a simple first-order Markov chain. To generate more complex and coherent text, consider implementing a higher-order Markov chain (considering two or more preceding words).\n",
        "*   The model currently handles only lowercase alphabetical words. Future work could involve handling punctuation, capitalization, and a wider range of characters to improve the naturalness of the generated text.\n"
      ]
    }
  ]
}